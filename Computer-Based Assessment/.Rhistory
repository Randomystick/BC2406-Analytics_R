# However, for Age, Weight, and NumMajorSurgeries, we need to standardise their coefficients first.
## Age
m_linreg2$coefficients[2] * sd(dt$Age) / sd(dt$Premium)
0.7368393
## Weight
m_linreg2$coefficients[6] * sd(dt$Weight) / sd(dt$Premium)
0.1586084
## NumMajorSurgeries
m_linreg2$coefficients[8] * sd(dt$NumMajorSurgeries) / sd(dt$Premium)
-0.07365812
################ Gender ################
100*m_cont_CART$variable.importance[11]/sum(m_cont_CART$variable.importance)
'Gender variable importance (unpruned CART): 1.17%'
summary(m_linreg)
'Gender p-value     (unoptimised linreg): 0.827232'
'Gender coefficient (unoptimised linreg): -2.6276'
'Gender Boxplot: refer to line 130'
################ BMI ################
100*m_cont_CART$variable.importance[4]/sum(m_cont_CART$variable.importance)
'BMI variable importance (unpruned CART): 6.5%'
100*m_cont_CART2$variable.importance[5]/sum(m_cont_CART2$variable.importance)
'BMI variable importance (pruned CART): 2.3%'
summary(m_cont_CART2)
'Pruned CART tree: refer to line 208 / Figure 4.2'
'BMI multicollinearity: refer to line 245'
'BMI not in stepped model: refer to line 253'
summary(m_linreg)
'BMI p-value     (unoptimised linreg): 0.533763'
'BMI coefficient (unoptimised linreg): -5.8927'
m_linreg_noWeight = lm(Premium ~ .-Weight, data = dt)
summary(m_linreg_noWeight)
vif(m_linreg_noWeight)
'Linreg without Weight R2: 0.6388, no multicollinearity'
m_linreg_noBMI = lm(Premium ~ .-BMI, data = dt)
summary(m_linreg_noBMI)
vif(m_linreg_noBMI)
'Linreg without BMI R2: 0.6397, no multicollinearity'
## Train-test Split ----
set.seed(123)
# 70% trainset. Stratify on Y = dt$Premium
train <- sample.split(Y = dt$Premium, SplitRatio = 0.7)
trainset <- subset(dt, train == T)
testset <- subset(dt, train == F)
# Checking the distribution of Y is similar in trainset vs testset.
summary(trainset$Premium)
summary(testset$Premium)
################ Continuous CART ################
set.seed(123)
m_cont_CART_tt <- rpart(Premium ~ ., data = trainset, method = 'anova', control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m_cont_CART_tt$cptable[which.min(m_cont_CART_tt$cptable[,"xerror"]), "xerror"] +
m_cont_CART_tt$cptable[which.min(m_cont_CART_tt$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cont_CART_tt$cptable[i,j] > CVerror.cap){
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cont_CART_tt$cptable[i,1] * m_cont_CART_tt$cptable[i-1,1]), 1)
m_cont_CART_tt <- prune(m_cont_CART_tt, cp = cp.opt)
sqrt(mean(residuals(m_cont_CART_tt)^2))
176.8036
pred_CART <- predict(m_cont_CART_tt,testset)
sqrt(mean((testset$Premium-pred_CART)^2))
157.5959
################ Linear Regression ################
m_linreg_tt <- lm(Premium ~ ., data = trainset)
m_linreg_tt <- step(m_linreg_tt)
summary(m_linreg_tt)
vif(m_linreg_tt) #no multicollinearity
sqrt(mean(residuals(m_linreg_tt)^2))
187.9428
pred_LINREG <- predict(m_linreg_tt,testset)
sqrt(mean((testset$Premium-pred_LINREG)^2))
sqrt(mean(residuals(m_linreg_tt)^2))
sqrt(mean(residuals(m_linreg_tt)^2))
summary(m_linreg_tt)
################ Linear Regression ################
m_linreg_tt <- lm(Premium ~ ., data = trainset)
m_linreg_tt <- step(m_linreg_tt)
summary(m_linreg_tt)
vif(m_linreg_tt) #no multicollinearity
sqrt(mean(residuals(m_linreg_tt)^2))
188.3476
pred_LINREG <- predict(m_linreg_tt,testset)
sqrt(mean((testset$Premium-pred_LINREG)^2))
185.5398
sqrt(mean(residuals(m_cont_CART_tt)^2))
176.8036
pred_CART <- predict(m_cont_CART_tt,testset)
sqrt(mean((testset$Premium-pred_CART)^2))
157.5959
################ Linear Regression ################
m_linreg_tt <- lm(Premium ~ ., data = trainset)
m_linreg_tt <- step(m_linreg_tt)
summary(m_linreg_tt)
vif(m_linreg_tt) #no multicollinearity
sqrt(mean(residuals(m_linreg_tt)^2))
188.3476
pred_LINREG <- predict(m_linreg_tt,testset)
sqrt(mean((testset$Premium-pred_LINREG)^2))
185.5398
###################################################  Question 5  ###################################################
####################################  Explain the limitations of your analysis.  ###################################
summary(dtCat$Premium)
par(mfrow = c(2,2))
plot(m_linreg2)
par(mfrow = c(1,1))  # Reset plot options to 1 chart in one plot.
mean(dt$NumMajorSurgeries)
m_linreg2$coefficients[8]
train <- sample.split(Y = dtCat$Premium, SplitRatio = 0.7)
trainset <- subset(dtCat, train == T)
testset <- subset(dtCat, train == F)
m_cate_CART <- rpart(Premium ~ ., data = trainset, method = 'class', control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xerror"] + m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cate_CART$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cate_CART$cptable[i,1] * m_cate_CART$cptable[i-1,1]), 1)
m_cate_CART <- prune(m_cate_CART, cp=cp.opt)
pred <- predict(m_cate_CART, testset, type = "class")
table('Predicted' = pred, 'Actual' = testset$Premium)
pred <- predict(m_cate_CART, testset, type = "class")
table('Predicted' = pred, 'Actual' = testset$Premium)
table('Predicted' = pred, 'Actual' = testset$Premium)
m_cate_CART <- rpart(Premium ~ ., data = dt, method = 'class', control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xerror"] + m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cate_CART$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cate_CART$cptable[i,1] * m_cate_CART$cptable[i-1,1]), 1)
m_cate_CART <- prune(m_cate_CART, cp=cp.opt)
pred <- predict(m_cate_CART, dt, type = "class")
table('Predicted' = pred, 'Actual' = testset$Premium)
pred <- predict(m_cate_CART, dt, type = "class")
table('Predicted' = pred, 'Actual' = dt$Premium)
pred <- predict(m_cate_CART, dt, type = "class")
table('Predicted' = pred, 'Actual' = dt$Premium)
m_cate_CART <- rpart(Premium ~ ., data = dt, method = 'class', control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xerror"] + m_cate_CART$cptable[which.min(m_cate_CART$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cate_CART$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cate_CART$cptable[i,1] * m_cate_CART$cptable[i-1,1]), 1)
m_cate_CART <- prune(m_cate_CART, cp=cp.opt)
pred <- predict(m_cate_CART, dt, type = "class")
table('Predicted' = pred, 'Actual' = dt$Premium)
#########################################  B C 2 4 0 6   S E M I N A R   2  ########################################
##########################################  C B A (Bryan Leow Ken Hing)  ###########################################
######################################### Instructor: Prof Vivek Choudhary #########################################
setwd("C:/Users/bryan/OneDrive - Nanyang Technological University/BC2406 ANALYTICS I/AY21S1 CBA")
library(data.table)     # fread
library(rpart)          # CART
library(rpart.plot)     # enhanced tree plots
library(ggplot2)        # plots
library(car)            # multicollinearity
library(caTools)        # train-test split
dt <- fread("premium2.csv")
###################################################  Question 1  ###################################################
########################  Create the BMI variable based on CDC definition. Show your code.  ########################
if (2.72 < mean(dt$Height) & (mean(dt$Height) < 8.92)) { # in imperial units
dt$BMI = (dt$Weight/2.20462) * (dt$Height*30.48/100)^2          # convert to metric for calculation
} else if (8.93 < mean(dt$Height) & (mean(dt$Height) < 250)) {  # in metric units, cm
dt$BMI = dt$Weight / (dt$Height/100)^2
} else if (250 < mean(dt$Height)) {                             # in metric units, m
dt$BMI = dt$Weight / (dt$Height)^2
}
dt$BMI <- round(dt$BMI, digits = 1)
summary(dt$BMI)
###################################################  Question 2  ###################################################
#########################  There are many categorical variables with integer coded values  #########################
##############################  (e.g. Diabetes, HighBloodPressure, Transplantâ€¦etc.)  ###############################
############################  Is it necessary to convert them to factor datatype in R?  ############################
class(dt$Diabetes)
summary(dt$Diabetes)
#### LIN REG ####
m_linreg <- lm(Height ~ ., data = dt)
summary(m_linreg)
# LIN REG Results - coded as integer
sqrt(c(crossprod(m_linreg$residuals)) / length(m_linreg$residuals))
class(dt$Diabetes)
#### LOG REG ####
m_logreg <- glm(Diabetes ~ ., family = "binomial", data = dt)
summary(m_logreg)
prob <- predict(m_logreg, dt, type = 'response')
threshold <- 0.5
pred_m_logreg <- ifelse(prob > threshold, 1, 0)
# LOG REG RESULTS - coded as integer
table("predicted" = pred_m_logreg, "actual" = dt$Diabetes, deparse.level = 2)
class(dt$Diabetes)
#### CART ####
m_cart <- rpart(Diabetes ~ ., data = dt, method = 'anova', control = rpart.control(minsplit = 2, cp = 0))
printcp(m_cart)
plotcp(m_cart)
CVerror.cap <- m_cart$cptable[which.min(m_cart$cptable[,"xerror"]), "xerror"] + m_cart$cptable[which.min(m_cart$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cart$cptable[i,j] > CVerror.cap) {
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cart$cptable[i,1] * m_cart$cptable[i-1,1]), 1)
cp.opt
m_cart <- prune(m_cart, cp=cp.opt)
pred_m_cart = predict(m_cart, dt) #, type = 'class')
##sqrt(mean((pred_m_cart-dt$Diabetes)^2))
# CART RESULTS - coded as integer, method = 'anova'
table("predicted" = pred_m_cart, "actual" = dt$Diabetes, deparse.level = 2)
class(dt$Diabetes)
#### convert to factor ####
dt$Diabetes             <- as.factor(dt$Diabetes)
dt$HighBloodPressure    <- as.factor(dt$HighBloodPressure)
dt$Transplant           <- as.factor(dt$Transplant)
dt$ChronicDisease       <- as.factor(dt$ChronicDisease)
dt$Allergy              <- as.factor(dt$Allergy)
dt$CancerInFamily       <- as.factor(dt$CancerInFamily)
dt$Gender               <- as.factor(dt$Gender)
rm(m_cart, m_linreg, m_logreg)
###################################################  Question 3  ###################################################
###############################  Explore the data and report on your key findings.  ################################
#### Univariate: Premium ####
ggplot(dt) + aes(x=Premium) + geom_area(stat="bin", fill="lightblue", bins = 30) + labs(x = "Premium", y = "Count", title = "Distribution of Premium, Histogram")
ggplot(dt) + aes(x=Premium) + geom_boxplot(width=.2, fill="pink") + labs(x = "Premium", title = "Distribution of Premium, Boxplot")
summary(dt$Premium)
length(unique(dt$Premium))
#### Bivariate: Premium-Continuous ####
ggplot(dt, aes(x=Age, y=Premium)) +
geom_point() + geom_smooth() +
labs(title = "Premium vs Age")
ggplot(dt, aes(x=Weight, y=Premium)) +
geom_point() + geom_smooth() +
labs(title = "Premium vs Weight")
ggplot(dt, aes(x=Height, y=Premium)) +
geom_point() + geom_smooth() +
labs(title = "Premium vs Height")
ggplot(dt, aes(x=NumMajorSurgeries, y=Premium)) +
geom_point() + geom_smooth() +
labs(title = "Premium vs NumMajorSurgeries")
#### Categorical Variables ####
dtViz = dt
levels(dtViz$Diabetes)          = c("Absent", "Present")
levels(dtViz$HighBloodPressure) = c("Absent", "Present")
levels(dtViz$Transplant)        = c("No", "Yes")
levels(dtViz$ChronicDisease)    = c("No", "Yes")
levels(dtViz$Allergy)           = c("No", "Yes")
levels(dtViz$CancerInFamily)    = c("No", "Yes")
levels(dtViz$Gender)            = c("Female", "Male")
#### Bivariate: Premium-Categorical ####
ggplot(dtViz, mapping = aes(x = Premium, y = Diabetes, fill=Diabetes)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Diabetes", y="Diabetes")
ggplot(dtViz, mapping = aes(x = Premium, y = HighBloodPressure, fill=HighBloodPressure)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs High Blood Pressure", y="High Blood Pressure")
ggplot(dtViz, mapping = aes(x = Premium, y = Transplant, fill=Transplant)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Transplant", y="Transplant")
ggplot(dtViz, mapping = aes(x = Premium, y = ChronicDisease, fill=ChronicDisease)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Chronic Disease", y="Chronic Disease")
ggplot(dtViz, mapping = aes(x = Premium, y = Allergy, fill=Allergy)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Allergy", y="Allergy")
ggplot(dtViz, mapping = aes(x = Premium, y = CancerInFamily, fill=CancerInFamily)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Cancer In Family", y="Cancer In Family")
ggplot(dtViz, mapping = aes(x = Premium, y = Gender, fill=Gender)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Premium vs Gender", y="Gender")
#### Univariate: Premium as categorical ####
dtCat <- dt
dtCat$Premium <- as.factor(dt$Premium)
ggplot(dtCat) + aes(x=Premium, y=(..count..), fill=(..count..)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Premium", y = "Count of Premium", title = "Distribution of Premium, Barplot")
summary(dtCat$Premium)
#### Univariate: Categorical variables ####
ggplot(dtViz, aes(x = Diabetes, fill = Diabetes)) +
geom_bar() +
scale_fill_manual(values = c("Absent" = "lightblue", "Present" = "pink")) +
labs(title = "Incidence of Diabetes")
ggplot(dtViz, aes(x = HighBloodPressure, fill = HighBloodPressure)) +
geom_bar() +
scale_fill_manual(values = c("Absent" = "lightblue", "Present" = "pink")) +
labs(title = "Incidence of High Blood Pressure")
ggplot(dtViz, aes(x = Transplant, fill = Transplant)) +
geom_bar() +
scale_fill_manual(values = c("No" = "lightblue", "Yes" = "pink")) +
labs(title = "Incidence of Transplant")
ggplot(dtViz, aes(x = ChronicDisease, fill = ChronicDisease)) +
geom_bar() +
scale_fill_manual(values = c("No" = "lightblue", "Yes" = "pink")) +
labs(title = "Incidence of Chronic Disease")
ggplot(dtViz, aes(x = Allergy, fill = Allergy)) +
geom_bar() +
scale_fill_manual(values = c("No" = "lightblue", "Yes" = "pink")) +
labs(title = "Incidence of Allergy")
ggplot(dtViz, aes(x = CancerInFamily, fill = CancerInFamily)) +
geom_bar() +
scale_fill_manual(values = c("No" = "lightblue", "Yes" = "pink")) +
labs(title = "Incidence of Cancer In Family")
ggplot(dtViz, aes(x = Gender, fill = Gender)) +
geom_bar() +
scale_fill_manual(values = c("Female" = "pink", "Male" = "lightblue")) +
labs(title = "Distribution of Gender")
#### Age-Categorical ####
ggplot(dtViz, mapping = aes(x = Age, y = Diabetes, fill=Diabetes)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Diabetes", y="Diabetes")
ggplot(dtViz, mapping = aes(x = Age, y = HighBloodPressure, fill=HighBloodPressure)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs High Blood Pressure", y="High Blood Pressure")
ggplot(dtViz, mapping = aes(x = Age, y = Transplant, fill=Transplant)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Transplant", y="Transplant")
ggplot(dtViz, mapping = aes(x = Age, y = ChronicDisease, fill=ChronicDisease)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Chronic Disease", y="Chronic Disease")
ggplot(dtViz, mapping = aes(x = Age, y = Allergy, fill=Allergy)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Allergy", y="Allergy")
ggplot(dtViz, mapping = aes(x = Age, y = CancerInFamily, fill=CancerInFamily)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Cancer In Family", y="Cancer In Family")
ggplot(dtViz, mapping = aes(x = Age, y = Gender, fill=Gender)) +
geom_boxplot(show.legend = TRUE) +
labs(title = "Age vs Gender", y="Gender")
# weight and height ggplot codes truncated as they are similar to that for age.
###################################################  Question 4  ###################################################
#####################  Using 1 SE optimal CART and one other technique learnt in this course:  #####################
##################################################  Question 4a.  ##################################################
#############  What is the 10-fold cross validation RMSE and number of splits in the 1SE Optimal CART?  ############
################ Continuous CART ################
## For randomisation in 10-fold CV ----
set.seed(123)
## Generate max model ----
# rpart() completes phase 1 & 2 automatically.
# Change two default settings in rpart: minsplit and cp.
m_cont_CART <- rpart(Premium ~ ., data = dt, method = 'anova', control = rpart.control(minsplit = 2, cp = 0))
# MINSPLIT: the min number of observations that must exist in a node to attempt a split. default is 20. changed to 2 due to small sample size.
# CP: default is 0.01. changed to 0 to ensure grow tree to the max in phase 1.
## Print maximal tree ----
print(m_cont_CART)
# 512) Height>=163 51 0.000  750.0000 *
# node number: 512 (terminal node)
# decision rule: Height>=163 and all other nodes that led to it
# total 51 cases in the node
# deviance: 0
# predicted value: 750
rpart.plot(m_cont_CART, nn = T, main = "Continuous CART, unpruned")
## Print pruning sequence and 10-fold CV errors as table ----
printcp(m_cont_CART)
# CP: prune triggers
# rel error                  \
# xerror: 10-fold CV error    |> are PERCENTAGES of the root node. 1.000 -> 100% of the root node = 0.41935
# xstd                       /
# pruning sequence read from bottom up. recall: if CP=0, tree is max
## Display the pruning sequence and 10-fold CV errors as chart ----
plotcp(m_cont_CART)
#                   |
#                   |
# 10-fold CV error  |
#                   |
#                   |
#                   |
#                   _______________________________________
#                         cp BASED ON GEOMETRIC MEAN
#                         because there is a range of cp values for which there will be no pruning
#                         geometric mean = minVal * maxVal
# 9th tree is optimal. Choose any CP value betw the 8th and 9th tree CP values.
cp1 <- sqrt(1.3421e-02*1.0940e-02)
## Using formula method to extract the Optimal Tree ----
# Compute min CVerror + 1SE in maximal tree.
CVerror.cap <- m_cont_CART$cptable[which.min(m_cont_CART$cptable[,"xerror"]), "xerror"] +
m_cont_CART$cptable[which.min(m_cont_CART$cptable[,"xerror"]), "xstd"]
# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree cart1.
i <- 1; j<- 4
while (m_cont_CART$cptable[i,j] > CVerror.cap){
i <- i + 1
}
# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m_cont_CART$cptable[i,1] * m_cont_CART$cptable[i-1,1]), 1)
# i = 9 shows that the 9th tree is indeed optimal based on 1 SE rule.
# cp.opt is the same as cp1 (difference due to rounding off error.)
## Prune the tree ----
m_cont_CART2 <- prune(m_cont_CART, cp = cp.opt)
rpart.plot(m_cont_CART2, nn = T, main = "Continuous CART, pruned")
## --- Trainset Error & CV Error --------------------------
printcp(m_cont_CART2)
## Root node error: 96355891/988 = 97526
## m_cont_CART trainset MSE = rel error * root node error = 0.23498 * 97526
## m_cont_CART CV MSE       = xerror    * root node error = 0.26369 * 97526
## RMSE
sqrt(0.26369 * 97526)
## nsplit
8
##################################################  Question 4b.  ##################################################
#####################################  Identify the key predictors of premium.  ####################################
################ Continuous CART ################
round(100*m_cont_CART2$variable.importance/sum(m_cont_CART2$variable.importance))
'Age';    'Transplant';    'Weight'
70;       11;               6
'NumMajorSurgeries'; 'BMI';    'ChronicDisease';    'CancerInFamily';    'HighBloodPressure';    'Diabetes'
4;                    2;        2;                   2;                   1;                      1
################ Linear Regression ################
## Generate linear regression model with all variables ----
m_linreg <- lm(Premium ~ ., data = dt)
summary(m_linreg)
# Very significant:
## Age, Transplant, ChronicDisease, CancerInFamily, NumMajorSurgeries
# Somewhat significant:
## Diabetes, Weight
# Not significant:
## Gender, BMI, HighBloodPressure, Height, Allergy
# note: the categorical variables have been dummy encoded by R such that the coefficient represents the
# effect of the variable if it is PRESENT and has a '1' at the back, but we will omit this '1' in our comments
## Removing multicollinearity ----
vif(m_linreg)
# Variables with multicollinearity:
## Weight, Height, BMI
## Which one to remove? Based on variable significance, might be Height and BMI
## Let's use step() function to confirm
## Perform backward elimination using Akaike Information Criterion, AIC ----
# Using the step function to determine which variables that we can remove
stepped_model <- step(m_linreg)
# According to step function, we can remove:
# Gender, BMI, HighBloodPressure, Height, Allergy
# This makes sense, since these variables are not significant based on their p-value.
# These are also the multicollinear variables.
m_linreg2 <- stepped_model
summary(m_linreg2)
vif(m_linreg2)
# All values are less than 5. No more multicollinearity.
# Very significant:
'Age'; 'Transplant'; 'ChronicDisease'; 'CancerInFamily'; 'NumMajorSurgeries'; 'Weight'
# Somewhat significant:
'Diabetes';
## Standardising the coefficients for continuous variables ----
# It is straightforward to compare the coefficients of categorical variables.
# However, for Age, Weight, and NumMajorSurgeries, we need to standardise their coefficients first.
## Age
m_linreg2$coefficients[2] * sd(dt$Age) / sd(dt$Premium)
0.7368393
## Weight
m_linreg2$coefficients[6] * sd(dt$Weight) / sd(dt$Premium)
0.1586084
## NumMajorSurgeries
m_linreg2$coefficients[8] * sd(dt$NumMajorSurgeries) / sd(dt$Premium)
-0.07365812
##################################################  Question 4c.  ##################################################
###############################  Is BMI or Gender important in determining premium?  ###############################
################ Gender ################
100*m_cont_CART$variable.importance[11]/sum(m_cont_CART$variable.importance)
'Gender variable importance (unpruned CART): 1.17%'
summary(m_linreg)
'Gender p-value     (unoptimised linreg): 0.827232'
'Gender coefficient (unoptimised linreg): -2.6276'
'Gender Boxplot: refer to line 130'
################ BMI ################
100*m_cont_CART$variable.importance[4]/sum(m_cont_CART$variable.importance)
'BMI variable importance (unpruned CART): 6.5%'
100*m_cont_CART2$variable.importance[5]/sum(m_cont_CART2$variable.importance)
'BMI variable importance (pruned CART): 2.3%'
summary(m_cont_CART2)
'Pruned CART tree: refer to line 208 / Figure 4.2'
'BMI multicollinearity: refer to line 245'
'BMI not in stepped model: refer to line 253'
summary(m_linreg)
'BMI p-value     (unoptimised linreg): 0.533763'
'BMI coefficient (unoptimised linreg): -5.8927'
m_linreg_noWeight = lm(Premium ~ .-Weight, data = dt)
summary(m_linreg_noWeight)
vif(m_linreg_noWeight)
'Linreg without Weight R2: 0.6388, no multicollinearity'
m_linreg_noBMI = lm(Premium ~ .-BMI, data = dt)
summary(m_linreg_noBMI)
vif(m_linreg_noBMI)
'Linreg without BMI R2: 0.6397, no multicollinearity'
##################################################  Question 4d.  ##################################################
#########  Evaluate and compare the predictive accuracy of the two techniques on a 70-30 train-test split  #########
####################################  Present testset RMSE results in a table.  ####################################
## Train-test Split ----
set.seed(123)
# 70% trainset. Stratify on Y = dt$Premium
train <- sample.split(Y = dt$Premium, SplitRatio = 0.7)
trainset <- subset(dt, train == T)
testset <- subset(dt, train == F)
# Checking the distribution of Y is similar in trainset vs testset.
summary(trainset$Premium)
summary(testset$Premium)
################ Continuous CART ################
set.seed(123)
m_cont_CART_tt <- rpart(Premium ~ ., data = trainset, method = 'anova', control = rpart.control(minsplit = 2, cp = 0))
CVerror.cap <- m_cont_CART_tt$cptable[which.min(m_cont_CART_tt$cptable[,"xerror"]), "xerror"] +
m_cont_CART_tt$cptable[which.min(m_cont_CART_tt$cptable[,"xerror"]), "xstd"]
i <- 1; j<- 4
while (m_cont_CART_tt$cptable[i,j] > CVerror.cap){
i <- i + 1
}
cp.opt = ifelse(i > 1, sqrt(m_cont_CART_tt$cptable[i,1] * m_cont_CART_tt$cptable[i-1,1]), 1)
m_cont_CART_tt <- prune(m_cont_CART_tt, cp = cp.opt)
sqrt(mean(residuals(m_cont_CART_tt)^2))
sqrt(mean(residuals(m_cont_CART_tt)^2))
176.8036
pred_CART <- predict(m_cont_CART_tt,testset)
sqrt(mean((testset$Premium-pred_CART)^2))
157.5959
################ Linear Regression ################
m_linreg_tt <- lm(Premium ~ ., data = trainset)
m_linreg_tt <- step(m_linreg_tt)
summary(m_linreg_tt)
vif(m_linreg_tt) #no multicollinearity
sqrt(mean(residuals(m_linreg_tt)^2))
188.3476
pred_LINREG <- predict(m_linreg_tt,testset)
sqrt(mean((testset$Premium-pred_LINREG)^2))
